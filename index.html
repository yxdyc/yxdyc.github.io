<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Daoyuan Chen's Homepage</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i, Raleway:300,300i,400,400i,500,500i,600,600i,700,700i, Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <style>
    li {
      margin: 10px 10px 10px 10px;
    }
  </style>
<style>
  .note {
    background-color: #f9f9f9;
    border-left: 4px solid #333;
    padding: 10px;
    margin: 15px 0;
    font-style: italic;
    color: #333;
  }
</style>

  <style>
    blockquote {
      border-left: 5px solid #ccc;
      padding-left: 15px;
      margin: 20px 0;
      font-style: italic;
      color: #555;
    }

    blockquote p {
      margin: 0;
    }
  </style>

  <style>
.filter-controls {
  margin-bottom: 20px;
}

.filter-controls button {
  margin-right: 10px;
  padding: 5px 15px;
  cursor: pointer;
}

.filter-controls button.active {
  background-color: #007bff;
  color: white;
}

/* .tag {
  display: inline-block;
  padding: 2px 8px;
  margin: 2px;
  border-radius: 12px;
  background-color: #e9ecef;
  cursor: pointer;
} */

.tag {
  margin: 0.2em;
  padding: 0.3em 0.6em;
  border-radius: 3px;
  cursor: pointer;
  display: inline-block;
  background-color: #f0f0f0;
}


.tag.active {
  background-color: #007bff;
  color: white;
}

.paper-item {
  margin-bottom: 20px;
  display: none;
}

.paper-item.visible {
  display: block;
}
/* 
#paper-list > h3 {
  margin-top: 2em;
  margin-bottom: 1em;
} */

.paper-container {
  margin-bottom: 1.5em;
  display: none; /* 默认隐藏 */
}

.paper-container.visible {
  display: block; /* 显示时可见 */
}

.paper-tags {
  margin-top: 0.5em;
  color: #666;
}

.paper-tags small {
  font-style: italic;
}

</style>


  <!-- =======================================================
    Modified based on the following template.
  * Template Name: Selecao - v4.6.0
  * Template URL: https://bootstrapmade.com/selecao-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>


<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center  header-transparent ">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">


        <h6><a href="https://yxdyc.github.io/" target="blank_" style="color: #ffffff; font-weight: bold;">Daoyuan Chen
            (陈道源)</a></h6>
      </div>

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#about" style="font-weight: bold;">About</a></li>
          <li><a class="nav-link scrollto" href="#research" style="font-weight: bold;">Selected Technical Works</a></li>
          <li><a class="nav-link scrollto" href="#experience" style="font-weight: bold;">Working Experiences</a></li>
          <li><a class="nav-link scrollto" href="#activities" style="font-weight: bold;">Professional Activities</a>
          </li>
          <li><a class="nav-link scrollto" href="#misc" style="font-weight: bold;">Misc.</a></li>
          <li><a class="nav-link scrollto" href="#contact" style="font-weight: bold;">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Header Wrapper Section ======= -->
  <section id="hero" class="d-flex flex-column justify-content-end align-items-center">
    <style>
      #hero {
        height: 10vh;
        padding-top: 5px;
        padding-bottom: 0px;
      }

      .hero-waves {
        height: 80px;
      }
    </style>
    <svg class="hero-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
      viewBox="0 24 150 28 " preserveAspectRatio="none">
      <defs>
        <path id="wave-path" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z">
      </defs>
      <g class="wave1">
        <use xlink:href="#wave-path" x="50" y="3" fill="rgba(173, 216, 230, 0.3)"></use> <!-- 浅蓝色 -->
      </g>
      <g class="wave2">
        <use xlink:href="#wave-path" x="50" y="0" fill="rgba(135, 206, 250, 0.5)"></use> <!-- 天蓝色 -->
      </g>
      <g class="wave3">
        <use xlink:href="#wave-path" x="50" y="9" fill="#fff" transform="scale(1, 0.5)"></use>
      </g>
    </svg>
  </section><!-- End Header Wrapper -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title">
          <h2>About</h2>
          <p>Daoyuan Chen (陈道源)</p>
        </div>

        <div class="row content">
          <div class="col-lg-6 pt-4 pt-lg-0">
            <p>
              Hi, I'm doing research and development at Data Analytics and Intelligence Lab, Alibaba Tongyi.
              I earned my Master's degree in Computer Application Technology in June 2019 from Peking University,
              co-supervised by
              <a href="https://dblp.org/pid/01/8558-1.html">Ying Shen</a> and
              <a href="https://dblp.org/pid/64/9060.html">Kai Lei</a> (academic mentors), and <a
                href="https://sites.google.com/site/yaliangli/">Yaliang Li</a> (industry mentor).
            </p>
            <br>
            <p>
              I've published over 30 technical <a href=#research> papers</a>, more than 10 of which I've led as the
              first author and were
              presented at top-tier conferences such as ICML, NeurIPS, ICLR, SIGMOD, TPAMI, KDD, and SIGIR.
            </p>
            <br>
            <p>
              I’ve learned a lot from the open-source community and am glad to have the opportunity to deeply engage
              with several interesting open-source projects.
            </p>
            <ul>
              <p> As a maintainer and technical leader, I've contributed to: </p>
              <li>
                •&emsp;<strong><a href="https://github.com/modelscope/data-juicer">Data-Juicer: </a></strong> <iframe
                  src="https://img.shields.io/github/stars/modelscope/data-juicer" . frameborder="0" scrolling="0"
                  width="105" height="20" title="GitHub">
                </iframe> 
                <ul>
                  <li>
                    Data
                    processing for and with foundation models.
                  </li>
                </ul>
              </li>

              <li>
                •&emsp;<a href="https://github.com/modelscope/trinity-rft">Trinity-RFT: </a> <iframe
                  src="https://img.shields.io/github/stars/modelscope/trinity-rft" frameborder="0" scrolling="0"
                  width="105" height="20" title="GitHub">
                </iframe> 
                <ul>
                  <li>
                    Reinforcement fine-tuning towards self-evolving LLMs.
                  </li>
                </ul>
              </li>

              <p> As a contributor from the official team, I've also worked on: </p>
              <li>
                •&emsp;<a href="https://github.com/alibaba/FederatedScope">FederatedScope: </a> <iframe
                  src="https://img.shields.io/github/stars/alibaba/FederatedScope" frameborder="0" scrolling="0"
                  width="105" height="20" title="GitHub">
                </iframe> 
                <ul>
                  <li>
                    An easy-to-use FL platform.
                  </li>
                </ul>
              </li>

              <li>
                •&emsp;<a href="https://github.com/modelscope/agentscope">AgentScope:</a>
                <iframe src="https://img.shields.io/github/stars/modelscope/agentscope" frameborder="0" scrolling="0"
                  width="105" height="20" title="GitHub">
                </iframe> 
                <ul>
                  <li>
                    Agent-oriented programming for building LLM applications. </li>
                </ul>

              </li>
            </ul>
          </div>

          <div class="col-lg-6 pt-4 pt-lg-0">
            <p>My interests broadly lie in insight- and theory-informed research, simple yet effective systems, and
              real-world applications related to:</p>
            <ul>
              <li>•&emsp;Large Language Models (LLMs)</li>
              <li>•&emsp;Multimodal LLMs</li>
              <li>•&emsp;Efficient Machine Learning (ML)</li>
              <li>•&emsp;Data- and Knowledge-Driven ML</li>
              <li>•&emsp;Human-centric ML</li>
              <li>•&emsp;Federated Learning (FL)</li>
            </ul>

            <p>More specifically, including but not limited to:</p>
            <ul>
              <li>•&emsp;Data-model co-development: building dedicated infrastructures, and exploring
                <em>generalized</em> feedback signals between them
              </li>
              <li>•&emsp;Enhancing, scaling, and improving the understanding of data quality, diversity, and usability
              </li>
              <li>•&emsp;Synthetic data for model training and evaluation</li>
              <li>•&emsp;Building data agents powered by natural language for broad applications</li>
              <li>•&emsp;Better human-AI interaction: empathetic dialog, multimodal AIGC, personalized modeling,
                co-design of RLHF & RLAIF. </li>
              <li>•&emsp;On-device solutions via utilizing efficient models, and addressing privacy issues with FL</li>
            </ul>

            <p>
              <span style="color: red;">Collaborations are welcome; we're currently hiring full-time
                researchers/developers and self-motivated interns!</span>
              Feel free to <a href="#contact" style="font-weight: bold;">
                reach out</a> if you are interested.
            </p>


          </div>

        </div>
      </div>
    </section><!-- End About Section -->

    <!-- ======= Dynamic Research Paper List Script ======= -->

<script>
const tags = {
    'llm': 'Large Language Models',
    'vlm': 'Vision Language Models',
    'efficient': 'Efficient ML',
    'data': 'Data- and Knowledge-Driven ML',
    'human': 'Human-centric ML',
    'fl': 'Federated Learning',
    'bench': 'Benchmarks and Tutorials',
    'sys': 'Systems and Tools',
  };

document.addEventListener('DOMContentLoaded', function() {

  const tagFilters = document.getElementById('tag-filters');
  Object.entries(tags).forEach(([key, label]) => {
    const tag = document.createElement('span');
    tag.className = 'tag';
    tag.dataset.tag = key;
    tag.textContent = label;
    tag.addEventListener('click', toggleTag);
    tagFilters.appendChild(tag);
  });

  document.querySelectorAll('.filter-controls button').forEach(button => {
    button.addEventListener('click', function() {
      document.querySelectorAll('.filter-controls button').forEach(b => b.classList.remove('active'));
      this.classList.add('active');
      groupPapers(this.dataset.groupby);
    });
  });

  groupPapers('topic');
});

function toggleTag(e) {
  e.target.classList.toggle('active');
  filterPapers();
}

function updateFilterTags(groupBy) {
  const papers = document.querySelectorAll('.paper-item');
  const tagFilters = document.getElementById('tag-filters');
  tagFilters.innerHTML = ''; // 清空现有标签

  if (groupBy === 'topic') {
    // 话题分类保持原样
    Object.entries(tags).forEach(([key, label]) => {
      const count = Array.from(papers).filter(paper => 
        paper.dataset.tags && paper.dataset.tags.split(',').includes(key)
      ).length;
      
      const tag = document.createElement('span');
      tag.className = 'tag';
      tag.dataset.tag = key;
      tag.textContent = `${label} (${count})`;
      tag.addEventListener('click', toggleTag);
      tagFilters.appendChild(tag);
    });
    
  } else {
    // 为其他分类创建唯一值列表并计数
    const valueCounts = new Map();
    papers.forEach(paper => {
      const value = paper.dataset[groupBy];
      valueCounts.set(value, (valueCounts.get(value) || 0) + 1);
    });

    // 根据groupBy类型排序
    let sortedValues;
    if (groupBy === 'year') {
      // 年份从大到小排序
      sortedValues = Array.from(valueCounts.keys()).sort((a, b) => parseInt(b) - parseInt(a));
    } else {
      // 其他类别按字母顺序排序
      sortedValues = Array.from(valueCounts.keys()).sort();
    }

    sortedValues.forEach(value => {
      const tag = document.createElement('span');
      tag.className = 'tag';
      tag.dataset.tag = value;
      tag.textContent = `${value} (${valueCounts.get(value)})`;
      tag.addEventListener('click', toggleTag);
      tagFilters.appendChild(tag);
    });
  }
}

function groupPapers(groupBy) {
  const papers = document.querySelectorAll('.paper-item');
  const container = document.getElementById('paper-list');
  
  container.innerHTML = '';

  if (groupBy === 'topic') {
    const paperMap = new Map();
    
    papers.forEach(paper => {
      const paperTags = paper.dataset.tags ? paper.dataset.tags.split(',') : ['undefined'];
      const tagLabels = paperTags.map(tag => tags[tag] || tag);
      
      const paperContainer = document.createElement('div');
      paperContainer.className = 'paper-container';
      
      const paperContent = document.createElement('div');
      paperContent.className = 'paper-item';
      Array.from(paper.attributes).forEach(attr => {
        paperContent.setAttribute(attr.name, attr.value);
      });
      paperContent.innerHTML = paper.innerHTML;
      
      const tagsContainer = document.createElement('div');
      tagsContainer.className = 'paper-tags';
      tagsContainer.innerHTML = `<small>Topics: ${tagLabels.join(' | ')}</small>`;
      
      paperContainer.appendChild(tagsContainer);
      paperContainer.appendChild(paperContent);
      
      // 使用年份和内容作为排序键
      const sortKey = `${paper.dataset.year || '0000'}-${paper.innerHTML}`;
      paperMap.set(sortKey, paperContainer);
    });


    
    const groupList = document.createElement('ul');
    // 按年份降序排序
    Array.from(paperMap.entries())
      .sort((a, b) => b[0].localeCompare(a[0])) // 按sortKey排序（年份降序）
      .forEach(([_, paperContainer]) => {
        groupList.appendChild(paperContainer);
      });
    
    const groupTitle = document.createElement('h3');
    // groupTitle.textContent = 'Papers by Topics';
    groupTitle.textContent = '';
    container.appendChild(groupTitle);

    container.appendChild(groupList);

  } else {
    const groups = {};
    papers.forEach(paper => {
      const key = paper.dataset[groupBy];
      if (!groups[key]) {
        groups[key] = [];
      }
      const newPaper = document.createElement('div');
      newPaper.className = 'paper-item';
      Array.from(paper.attributes).forEach(attr => {
        newPaper.setAttribute(attr.name, attr.value);
      });
      newPaper.innerHTML = paper.innerHTML;
      groups[key].push({
        element: newPaper,
        year: parseInt(paper.dataset.year) || 0
      });
    });

    let sortedEntries;
    if (groupBy === 'year') {
      // 年份从大到小排序
      sortedEntries = Object.entries(groups).sort((a, b) => parseInt(b[0]) - parseInt(a[0]));
    } else {
      // 其他类别按字母顺序排序
      sortedEntries = Object.entries(groups).sort();
    }

    sortedEntries.forEach(([key, paperList]) => {
      const groupTitle = document.createElement('h3');
      groupTitle.textContent = `${key} (${paperList.length})`;
      container.appendChild(groupTitle);
      
      const groupList = document.createElement('ul');
      // 在每个组内按年份排序
      paperList
        .sort((a, b) => b.year - a.year) // 按年份降序排序
        .forEach(paper => groupList.appendChild(paper.element));
      container.appendChild(groupList);
    });
  }

  updateFilterTags(groupBy);
  filterPapers();
}

 function filterPapers() {
  const activeTags = Array.from(document.querySelectorAll('.tag.active')).map(tag => tag.dataset.tag);
  const currentGroupBy = document.querySelector('.filter-controls button.active').dataset.groupby;
  
  // 获取所有组标题和论文容器
  const groups = document.querySelectorAll('#paper-list > h3');
  const paperContainers = document.querySelectorAll('.paper-container, .paper-item');
  
  if (activeTags.length === 0) {
    // 如果没有选中的标签，显示所有内容
    groups.forEach(group => group.style.display = 'block');
    paperContainers.forEach(container => {
      container.classList.add('visible');
      if (container.classList.contains('paper-container')) {
        container.querySelector('.paper-item').classList.add('visible');
      }
    });
  } else {
    // 遍历每个组标题
    groups.forEach(groupTitle => {
      const nextElement = groupTitle.nextElementSibling;
      if (nextElement && nextElement.tagName === 'UL') {
        // 检查这个组下是否有可见的论文
        const groupPapers = nextElement.querySelectorAll('.paper-container, .paper-item');
        let hasVisiblePapers = false;

        groupPapers.forEach(container => {
          const paper = container.classList.contains('paper-container') ? 
                       container.querySelector('.paper-item') : 
                       container;
          
          let isVisible;
          if (currentGroupBy === 'topic') {
            const paperTags = paper.dataset.tags ? paper.dataset.tags.split(',') : [];
            isVisible = activeTags.some(tag => paperTags.includes(tag));
          } else {
            const paperValue = paper.dataset[currentGroupBy];
            isVisible = activeTags.includes(paperValue);
          }
          
          container.classList.toggle('visible', isVisible);
          if (paper !== container) {
            paper.classList.toggle('visible', isVisible);
          }
          
          if (isVisible) {
            hasVisiblePapers = true;
          }
        });

        // 根据组内是否有可见的论文来决定是否显示组标题
        groupTitle.style.display = hasVisiblePapers ? 'block' : 'none';
      }
    });
  }
}

 </script>

    <!-- ======= Selected Research Section ======= -->
    <section id="research" class="faq">
      <div class="container">

        <div class="section-title">
          <h2>Selected</h2>
          <p>Technical Works
          </p>
        </div>
        <p>Full paper lists: <a href="https://scholar.google.com/citations?hl=en&amp;user=1GdfinUAAAAJ">Google Scholar</a> and
          <a href="https://dblp.org/pid/217/4891.html">DBLP</a>
        </p>

          <p  class="note">
              You can click the buttons below to switch between different paper-grouping methods and different subgroups. 
            </p>
          <p  class="note">
              The number in parentheses ('n') indicates the size of each subgroup; '#' indicates co-first author; '^' indicates industrial mentor to the first
          student author.
          </p>

  <div class="filter-controls mb-4">
    <button class="btn btn-outline-primary active" data-groupby="topic">Group by Topics</button>
    <button class="btn btn-outline-primary" data-groupby="conference">Group by Conference/Journal/Preprints</button>
    <button class="btn btn-outline-primary" data-groupby="year">Group by Year</button>
    <button class="btn btn-outline-primary" data-groupby="authors">Group by Contribution</button>
  </div>

    <div id="tag-filters" class="mb-4"></div>
    <span class="badge badge-pill badge-secondary active mr-2" data-tag="topic"></span>

        <!-- topic1: <h4><b>(Multimodal) LLMs, Data-Driven & Human-Centric ML</b></h4> -->
        <ul id="paper-list" class="group-container">

          <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2025" 
            data-authors="First author, first position"
            data-tags="vlm,llm,data,efficient,sys">
            (<strong><em>ICML’25</em></strong> <em style="color: rgb(102, 0, 255);">spotlight, top 2.6%</em>) <a
              href="https://arxiv.org/abs/2407.11784"><strong> Data-Juicer Sandbox: A
                Feedback-Driven Suite for Multimodal Data-Model Co-development</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, Jingren Zhou
              </li>
              <li><em>A new middleware links data and model feedback, enabling high performance and low cost verified in
                  broad tasks. <em style="color: rgb(102, 0, 255);">Top-1</em> in VBench leaderboard.</em></li>
            </ul>
          </li>
          
          <li class="paper-item" 
            data-conference="TPAMI, CVPR"  
            data-year="2025" 
            data-authors="First author, non-first position"
            data-tags="llm,vlm,data,bench">
              (<strong><em>TPAMI’25</strong>, survey</em>) <a href="https://arxiv.org/abs/2407.08583">
              <strong>The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development
                Perspective</strong>
            </a>
            <ul>
              <li>Zhen Qin, <strong>Daoyuan Chen</strong>^#, Wenhao Zhang, Liuyi Yao, Yilun Huang, Bolin Ding, Yaliang
                Li, Shuiguang Deng</li>
              <li><em>Highlighting the current state and potential of co-development between data and multi-modal
                  LLMs,
                  in a dual perspective.
                </em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="First author, non-first position"
            data-tags="llm,data,human,sys">
          (<em>arXiv’25</em>) <a href="https://arxiv.org/abs/2505.17826"><strong>
                Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language
                Models
              </strong></a>
            <ul>
              <li>Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, <strong>Daoyuan Chen</strong>#, Wenhao Zhang,
                Yuexiang
                Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou
              </li>
              <li><em> A new open-source project, exploraing how to process feedback data, and evolve large models in
                  dynamic enviroments.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="First author, non-first position"
            data-tags="vlm,data,bench">
          (<em>arXiv’25</em>) <a href="https://www.arxiv.org/abs/2505.16915"><strong>
                DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?</strong></a>
            <ul>
              <li>Qirui Jiao, <strong>Daoyuan Chen</strong>^#, Yilun Huang, Xika Lin, Ying Shen, Yaliang Li</li>
              <li><em>A synthetic benchmark revealing notable performance drops despite large models' proficiency with
                  short descriptions. </em>
              </li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="First author, non-first position"
            data-tags="vlm,llm,data">
          (<em>arXiv’25</em>) <a href="https://arxiv.org/abs/2503.09499"><strong>
                MindGym: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?</strong></a>
            <ul>
              <li>Zhe Xu, <strong>Daoyuan Chen</strong>^#, Zhenqing Ling, Yaliang Li, Ying Shen</li>
              <li><em>A new data synthesis method, which enables large models to self-synthesize high-quality,
                  low-variance data for efficient fine-tuning (16% gain on MathVision using only 400 samples). </em>
              </li>
            </ul>
          </li>


          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="First author, non-first position"
            data-tags="llm,data">
          (<em>arXiv’25</em>) <a href="https://www.arxiv.org/abs/2502.04380"><strong>Diversity as a Reward:
                Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data</strong></a>
            <ul>
              <li>Zhenqing Ling, <strong>Daoyuan Chen</strong>^#, Liuyi Yao, Qianli Shen, Yaliang Li, Ying Shen</li>
              <li><em>A theoretically informed method, which treats diversity as a reward, achieves new SOTA average
                  performance across 7 benchmarks on SOTA LLMs with domain-undetermined data.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="TPAMI, CVPR"  
            data-year="2025" 
            data-authors="Second Author"
            data-tags="vlm,data">
          (<strong><em>CVPR’25</em></strong>) <a href="https://arxiv.org/pdf/2408.04594">
              <strong>Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models</strong>
            </a>
            <ul>
              <li>Qirui Jiao, <strong>Daoyuan Chen</strong>^, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen</li>
              <li><em>A new method of contrastive data synthesis, creating a high-quality dataset that describes
                  object
                  differences focusing on fine-grained regions in images.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="First author, first position"
            data-tags="llm,vlm,data,sys,efficient">
          (<em>arXiv’24</em>) <a href="https://arxiv.org/abs/2501.14755"><strong> Data-Juicer 2.0: Cloud-Scale
                Adaptive Data Processing for and with Foundation Models</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yilun Huang, Xuchen Pan, Nana Jiang, Haibin Wang, Yilei Zhang, Ce Ge,
                Yushuo Chen,
                Wenhao Zhang, Zhijian Ma, Jun Huang, Wei Lin, Yaliang Li, Bolin Ding, Jingren Zhou</li>
              <li><em>Offering efficient multimodal data processing abilities with <em
                    style="color: rgb(102, 0, 255);">100+ operators</em>, and seamless scalability for foundation
                  models.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2024" 
            data-authors="First author, first position"
            data-tags="llm,data,sys,efficient">
          (<strong><em>SIGMOD’24</em></strong>) <a href="https://arxiv.org/abs/2309.02033"><strong>Data-Juicer: A
                One-Stop Data Processing System for Large Language Models</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao,
                Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou</li>
              <li><em>Providing open, versatile data processing abilities to ease the creation and evaluation of
                  diverse
                  data recipes for LLMs.
                </em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2024" 
            data-authors="First author, first position"
            data-tags="llm,data,sys,vlm,bench">
            (<strong><em>KDD’24, tutorial</em></strong>) <a
              href="https://modelscope.github.io/data-juicer/_static/tutorial_kdd24.html"><strong>Multi-modal Data
                Processing for Foundation Models: Practical Guidances and Use Cases</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yaliang Li, Bolin Ding, The Data-Juicer Team</li>
              <li><em>Discussing practical skills in multi-modal data processing, to efficiently handle data variety,
                  quality, and scale for foundation models.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2024" 
            data-authors="Second Author"
            data-tags="vlm,data,bench,human">
          (<em>arXiv’24, benchmark</em>) <a href="https://www.arxiv.org/abs/2502.04380">
              <strong>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic
                Benchmark Data</strong>
            </a>
            <ul>
              <li>Ting Zhou, <strong>Daoyuan Chen</strong>^, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</li>
              <li><em>A new benchmark to evaluate MLLMs on inner emotion recognition and outer behavioral
                  manifestations, advancing human-like understanding in video perception.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM"  
            data-year="2024" 
            data-authors="Second Author"
            data-tags="vlm,data,human">
          (<strong><em>COLING’24</em></strong>) <a href="https://arxiv.org/abs/2403.11236"><strong>ChartThinker: A
                Contextual Chain-of-Thought Approach to Optimized Chart Summarization</strong></a>
            <ul>
              <li>Mengsha Liu, <strong>Daoyuan Chen</strong>^, Yaliang Li, Guian Fang and Ying Shen.</li>
              <li><em>Optimizing chart summarization by combining contextual reasoning and deep analysis, achieving
                  superior performance across multiple metrics.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2024" 
            data-authors="Second Author"
            data-tags="vlm,data">
          (<em>arXiv’24</em>) <a href="https://arxiv.org/abs/2401.17981">
              <strong>From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection
                Information</strong>
            </a>
            <ul>
              <li>Qirui Jiao, <strong>Daoyuan Chen</strong>^, Yilun Huang, Yaliang Li, Ying Shen.</li>
              <li><em>Showing how adaptively fine-tuning MLLMs with textual detection information can boost
                  performance
                  via extensive experiments.
                </em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2024" 
            data-authors="Subsequent Author"
            data-tags="llm,data">
          (<em>arXiv’24</em>) <a href="https://arxiv.org/abs/2402.11505">
              <strong>A Bivariate Data Mixing Law for Language Model Pretraining</strong>
            </a>
            <ul>
              <li>Ce Ge, Zhijian Ma, <strong>Daoyuan Chen</strong>, Yaliang Li, Bolin Ding.</li>
              <li><em>A new bivariate data mixing law that models the joint scaling patterns of domain proportions and
                  data quantity in LLM pretraining.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2024" 
            data-authors="Subsequent Author"
            data-tags="vlm,data,human">
          (<em>arXiv’24</em>) <a href="https://arxiv.org/abs/2412.12888">
              <strong> ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction</strong>
            </a>
            <ul>
              <li>Zhongjie Duan, Qianyi Zhao, Cen Chen, <strong>Daoyuan Chen</strong>, Wenmeng Zhou, Yaliang Li,
                Yingda
                Chen</li>
              <li><em>Enhancing text-to-image generation by iteratively integrating synthesis and understanding
                  models,
                  improving aesthetics and efficiency without extra inference costs.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2024" 
            data-authors="Subsequent Author"
            data-tags="llm,sys">
          (<em>arXiv’24</em>) <a href="https://arxiv.org/abs/2402.14034">
              <strong>AgentScope: A Flexible yet Robust Multi-Agent Platform</strong>
            </a>
            <ul>
              <li>Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang,
                Yuexiang Xie, <strong>Daoyuan Chen</strong>, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng,
                Hongzhu Shi, Yaliang Li, Bolin Ding, Jingren Zhou.</li>
              <li><em>Offering open and broad abilities to build LLM-empowered multi-agent applications in an easier
                  way.</em></li>
            </ul>
          </li>
        <!-- </ul> -->
        <!-- topic2: <h4 id="federated-learning-on-device-personalization-systems"> FL+LLM, On-device & Personalized FL</h4> -->
        <!-- <ul style="list-style-type:none;"> -->

          <li class="paper-item" 
            data-conference="arXiv" 
            data-year="2025" 
            data-authors="Subsequent Author"
            data-tags="llm,bench">
          (<em>arXiv’25</em>) <a href="https://arxiv.org/abs/2506.02961">
              <strong>FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models
</strong>
            </a>
            <ul>
              <li>Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, <strong>Daoyuan Chen</strong>, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane.</li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2024" 
            data-authors="First author, non-first position"
            data-tags="llm,data,fl">
          (<strong><em>NeurIPS’24</em></strong>) <a href="https://arxiv.org/abs/2402.11505"><strong>Federated
                Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client
                Resources</strong></a>
            <ul>
              <li>Jiamu Bai, <strong>Daoyuan Chen</strong>^#, Bingchen Qian, Liuyi Yao, Yaliang Li.</li>
              <li><em>An aggregation scheme for FL of LLMs that dynamically adjusts LoRA ranks to harness the full
                  potential of diverse client resources, enhancing generalization, and validated on thousands of
                  tasks.</em></li>
            </ul>
          </li>

            <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2024" 
            data-authors="First author, non-first position"
            data-tags="llm,fl,efficient">
          (<strong><em>ICML’24</em></strong>) <a href="https://arxiv.org/abs/2312.06353">
              <strong>
                Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18
                Kilobytes</strong>
            </a>
            <ul>
              <li>Zhen Qin, <strong>Daoyuan Chen</strong>^, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng.
              </li>
              <li><em> A theory-informed method for federated full-parameter tuning of LLMs, which incurs &lt18KB
                  communication cost per round for a 3B LLM, meanwhile delivering SOTA accuracy.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2024" 
            data-authors="Second Author"
            data-tags="llm,fl,efficient">
          (<strong><em>KDD’24</em></strong>) <a href="https://arxiv.org/abs/2402.05926">
              <strong>
                On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
              </strong>
            </a>
            <ul>
              <li>Zhenqing Ling, <strong>Daoyuan Chen</strong>^, Liuyi Yao, Yaliang Li, Ying Shen.</li>
              <li><em>FedMeZO, a memory-efficient method for FL of LLMs that achieves faster convergence and reduced
                  GPU
                  memory usage, backed by theoretical analysis and empirical validation.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2024" 
            data-authors="Subsequent Author"
            data-tags="llm,fl,sys">
          (<strong><em>KDD’24</em></strong>) <a href="https://arxiv.org/abs/2309.00363"> 
            <strong>FederatedScope-LLM: A
              Comprehensive Package for Fine-tuning Large Language Models in Federated Learning </strong>
            </a>
            <ul>
              <li>Weirui Kuang, Bingchen Qian, Zitao Li, <strong>Daoyuan Chen</strong>, Dawei Gao, Xuchen Pan,
                Yuexiang
                Xie, Yaliang Li, Bolin Ding, Jingren Zhou.</li>
              <li><em>A new package designed to address the challenges of fine-tuning LLMs in FL.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE"
            data-year="2024" 
            data-authors="Subsequent Author"
            data-tags="fl">
          (<strong><em>TKDE’24</em></strong>) <a href="https://dl.acm.org/doi/10.1109/TKDE.2024.3482448">
              <strong>Is Sharing Neighbor Generator in Federated Graph Learning Safe? </strong>
            </a>
            <ul>
              <li>Liuyi Yao, Zhen Wang, Yuexiang Xie, Yaliang Li, Weirui Kuang, <strong>Daoyuan Chen</strong>, Bolin
                Ding.</li>
              <li><em>Sharing neighbor generators in graph FL poses privacy risks, as it enables data reconstruction
                  attacks, highlighting the need for defense strategies.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2023" 
            data-authors="First author, first position"
            data-tags="llm,fl,efficient">
          (<strong><em>ICML’23</em></strong>) <a href="https://arxiv.org/abs/2305.02776">
              <strong>Efficient Personalized Federated Learning via Sparse Model-Adaptation</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Liuyi Yao, Dawei Gao, Yaliang Li, Bolin Ding.</li>
              <li><em>An efficient pFL method with theoretical analysis, which adaptively learns sparse local models,
                  and achieves SOTA accuracy with improved efficiency simultaneously.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2023" 
            data-authors="First author, first position"
            data-tags="fl,efficient,sys">
          (<strong><em>KDD’23</em></strong>) <a href="https://arxiv.org/abs/2303.13363">
              <strong>FS-Real: Towards Real-World Cross-Device Federated Learning</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, Bolin
                Ding,
                Jingren Zhou.</li>
              <li><em>An efficient, scalable system for real-world cross-device FL, addressing challenges of
                  heterogeneous devices and large-scale training.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2023" 
            data-authors="First author, non-first position"
            data-tags="fl,efficient,sys">
          (<strong><em>VLDB’23</em></strong>) <a href="https://bolinding.github.io/papers/vldb23fsreal.pdf">
              <strong>FS-Real: A Real-World Cross-Device Federated Learning Platform</strong>
            </a>
            <ul>
              <li>Dawei Gao, <strong>Daoyuan Chen</strong>#, Zitao Li, Yuexiang Xie, Xuchen Pan, Yaliang Li, Bolin
                Ding,
                Jingren Zhou.</li>
              <li><em>Industrial use-cases of FS-Real, including phones and cars.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2023" 
            data-authors="Subsequent Author"
            data-tags="fl">
          (<strong><em>KDD’23</em></strong>) <a href="https://arxiv.org/abs/2302.01677">
              <strong>Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks</strong>
            </a>
            <ul>
              <li>Zeyu Qin, Liuyi Yao, <strong>Daoyuan Chen</strong>, Yaliang Li, Bolin Ding, Minhao Cheng.</li>
              <li><em>Personalized FL with partial model-sharing boosts robustness against backdoor attacks, unlike
                  full
                  model-sharing, offering insights for improved defense strategies.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2023" 
            data-authors="Subsequent Author"
            data-tags="fl,sys">
          (<strong><em>VLDB’23</em></strong>) <a href="https://arxiv.org/abs/2204.05011">
              <strong>FederatedScope: A Flexible Federated Learning Platform for Heterogeneity</strong>
            </a>
            <ul>
              <li>Yuexiang Xie, Zhen Wang, Dawei Gao, <strong>Daoyuan Chen</strong>, Liuyi Yao, Weirui Kuang, Yaliang
                Li, Bolin Ding, Jingren Zhou.</li>
              <li><em>An open package for FL research and development.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2022" 
            data-authors="First author, first position"
            data-tags="fl,efficient,bench">
          (<strong><em>NeurIPS’22, benchmark</em></strong>) <a href="https://arxiv.org/abs/2206.03655">
              <strong>pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Dawei Gao, Weirui Kuang, Yaliang Li, Bolin Ding.</li>
              <li><em>Benchmarking personalized FL, containing more than 10 datasets, 20 pFL methods, and systematic
                  evaluation with highlighted benefits and potential of pFL.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="KDD, SIGMOD, VLDB, TKDE" 
            data-year="2022" 
            data-authors="Subsequent Author"
            data-tags="fl,bench">  
          (<strong><em>KDD’22, tutorial</em></strong>) <a href="https://joneswong.github.io/KDD22FLTutorial/">
              <strong>A Practical Introduction to Federated Learning</strong>
            </a>
            <ul>
              <li>Yaliang Li, Bolin Ding, Zhen Wang, Yuexiang Xie, Dawei Gao, Liuyi Yao, <strong>Daoyuan
                  Chen</strong>,
                Weirui Kuang, Hongzhu Shi, Jingren Zhou</li>
              <li><em>Hands-on lessons on FL and FS package.</em></li>
            </ul>
          </li>
        <!-- </ul> -->

        <!-- <h4 id="efficient-machine-learning-adaptiveness-dynamics-applications">Efficient, Adaptive, & Knowledge-Driven
          ML</h4>
        <ul style="list-style-type:none;"> -->

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM" 
            data-year="2024" 
            data-authors="Second Author"
            data-tags="efficient,human">
          (<strong><em>SIGIR’24</em></strong>) <a href="https://arxiv.org/abs/2404.02505">
              <strong>Dynamic Demonstration
                Retrieval and Cognitive Understanding for Emotional Support Conversation</strong>
            </a>
            <ul>
              <li>Zhe Xu, <strong>Daoyuan Chen</strong>^, Jiayi Kuang, Zihao Yi, Yaliang Li, Ying Shen.</li>
              <li><em> Enhancing emotional support conversations by improving empathetic response generation and
                  comprehending implicit mental states.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="ICML, NeurIPS, ICLR" 
            data-year="2023" 
            data-authors="First author, first position"
            data-tags="efficient,sys">
          (<strong><em>ICLR’23</em></strong>) <a href="https://openreview.net/forum?id=UiaUEICawgw">
              <strong>Learned Index with Dynamic $\epsilon$</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Wuchao Li, Yaliang Li, Bolin Ding, Kai Zeng, Defu Lian, Jingren Zhou.
              </li>
              <li><em>A mathematically-grounded learned index framework, which is efficient, based on theoretically
                  derived prediction error bounds, and pluggable to several SOTA learned index
                  methods.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="TNNLS, IJCAI, AAAI, AIMedicine"  
            data-year="2023" 
            data-authors="Subsequent Author"
            data-tags="fl,efficient">
          (<em>TNNLS, 2023, IF 10.4</em>) <a href="https://pubmed.ncbi.nlm.nih.gov/34752410/">
              <strong>Knowledge-Based
                Reasoning Network for Relation Detection</strong>
            </a>
            <ul>
              <li>Ying Shen, Min Yang, Yaliang Li, Dong Wang, Haitao Zheng, <strong>Daoyuan Chen</strong>.</li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM" 
            data-year="2020" 
            data-authors="First author, first position"
            data-tags="efficient,data">
          (<strong><em>ACL’20</em></strong>) <a href="https://arxiv.org/abs/2004.09930">
              <strong>Relabel the noise: joint
                extraction of entities and relations via cooperative multiagents</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yaliang Li, Kai Lei, Ying Shen.</li>
              <li><em>A cooperative multiagent method jointly extracts entities and relations by re-labeling noisy
                  data,
                  addressing shifted label distribution and improving extraction performance.
                </em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="TNNLS, IJCAI, AAAI, AIMedicine"   
            data-year="2020" 
            data-authors="First author, first position"
            data-tags="efficient">
          (<strong><em>IJCAI’20</em></strong>) <a href="https://arxiv.org/abs/2001.04246">
              <strong>Adabert: Task-adaptive
                bert compression with differentiable neural architecture search</strong>
            </a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo
                Deng,
                Jun Huang, Wei Lin, Jingren Zhou.</li>
              <li><em>Using differentiable neural architecture search to compress BERT into task-adaptive models,
                  achieving faster inference and smaller size while maintaining performance.</em></li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM"  
            data-year="2020" 
            data-authors="First author, first position"
            data-tags="data">
          (<strong><em>CIKM’20</em></strong>) <a href="https://dl.acm.org/doi/10.1145/3340531.3411989">
              <strong>An Adaptive
                Embedding Framework for Heterogeneous Information Networks</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yaliang Li, Bolin Ding, Ying Shen.</li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="TNNLS, IJCAI, AAAI, AIMedicine"   
            data-year="2020" 
            data-authors="Subsequent Author"
            data-tags="efficient,data">
          (<strong><em>AAAI’20</em></strong>) <a href="https://arxiv.org/abs/1911.09801">
              <strong>Joint learning of answer
                selection and answer summary generation in community question answering</strong></a>
            <ul>
              <li>Yang Deng, Wai Lam, Yuexiang Xie, <strong>Daoyuan Chen</strong>, Yaliang Li, Min Yang, Ying Shen.
              </li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM"  
            data-year="2019" 
            data-authors="First author, first position"
            data-tags="data">(<strong><em>CIKM’19</em></strong>) <a href="https://dl.acm.org/doi/10.1145/3357384.3358071">
              <strong>Knowledge-aware textual entailment with graph
                attention network</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Yaliang Li, Min Yang, Hai-Tao Zheng, Ying Shen.</li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM" 
            data-year="2019" 
            data-authors="First author, first position"
            data-tags="data">(<strong><em>SIGIR’19</em></strong>) <a href="https://dl.acm.org/doi/abs/10.1145/3331184.3331328">
              <strong>Answer-enhanced Path-aware Relation Detection
                over Knowledge Base</strong></a>
            <ul>
              <li><strong>Daoyuan Chen</strong>, Min Yang, Hai-Tao Zheng, Yaliang Li, Ying Shen.</li>
            </ul>
          </li>
          
          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM"  
            data-year="2018" 
            data-authors="First author, non-first position"
            data-tags="efficient,data">(<strong>COLING’18</strong>, <strong><em style="color: rgb(102, 0, 255);"><a
                  href="https://coling2018.org/index.html%3Fp=1558.html" style="color: rgb(102, 0, 255);">best paper
                  nominations</a></em></strong>) <a href="https://aclanthology.org/C18-1036">
              <strong>Cooperative denoising
                for distantly supervised relation extraction</strong></a>
            <ul>
              <li>Kai Lei, <strong>Daoyuan Chen</strong>#, Yaliang Li, Nan Du, Min Yang, Wei Fan, Ying Shen.</li>
            </ul>
          </li>

          <li class="paper-item" 
            data-conference="SIGIR, ACL, COLING, CIKM" 
            data-year="2018" 
            data-authors="First author, non-first position"
            data-tags="data">(<strong><em>SIGIR’18</em></strong>)
            <a href="https://dl.acm.org/doi/abs/10.1145/3209978.3210067">
              <strong>Ontology
                evaluation with path-based text-aware entropy computation</strong>
            </a>
            <ul>
              <li>Ying Shen, <strong>Daoyuan Chen</strong>#, Min Yang, Yaliang Li, Nan Du, Kai Lei.</li>
            </ul>
          </li>
          
          <li class="paper-item" 
            data-conference="TNNLS, IJCAI, AAAI, AIMedicine"  
            data-year="2018" 
            data-authors="Subsequent Author"
            data-tags="data,human,system">(<em>Artificial intelligence in medicine, 2018, IF 7.5</em>) <a
              href="https://pubmed.ncbi.nlm.nih.gov/29433958/">
              <strong>An ontology-driven clinical decision support system
                (IDDAP) for infectious disease diagnosis and antibiotic prescription</strong>
            </a>
            <ul>
              <li>Ying Shen, Kaiqi Yuan, <strong>Daoyuan Chen</strong>, Joël Colloc, Min Yang, Yaliang Li, Kai Lei.
              </li>
            </ul>
          </li>
        </ul>
      </div>
    </section>

    <!-- ===== Dynamic Update on Paper Index ===== -->
    <!-- <script>
      const targetSection = document.querySelector('#research.faq');
      if (targetSection) {
        let globalIndex = 1;

        targetSection.querySelectorAll('ul > li').forEach(li => {
          if (li.parentElement.tagName === 'UL' && li.parentElement.style.listStyleType === 'none') {
            const itemId = `paper-${globalIndex}`;
            li.id = itemId;

            const link = document.createElement('a');
            link.href = `#${itemId}`;
            link.textContent = `[${globalIndex}] `;

            li.prepend(link);

            globalIndex++;
          }
        });
      }
    </script> -->
    <!-- Dynamic Update on Paper Index -->


    <!-- End Selected Research Section -->

    <!-- ======= Experience Section ======= -->
    <section id="experience" class="faq">
      <div class="container">
        <div class="section-title">
          <h2>Working</h2>
          <p>Experiences</p>
        </div>
        <ul>
          <li>2023 - Now, Data Analytics and Intelligence Lab, Alibaba Tongyi</li>
          <li>July 2019 - 2023, Data Analytics and Intelligence Lab, Alibaba DAMO Academy</li>
          <li>Research Intern, March 2018 - June 2018, Tencent Medical AI Lab</li>
          <li>Research Assistant, October 2016 - August 2017, Multimedia Software Engineering Research Center @ City
            University of Hong Kong</li>
        </ul>
      </div>
    </section>
    <!-- End Experience Section -->



    <!-- ======= activities Section ======= -->
    <section id="activities" class="faq">
      <div class="container">
        <div class="section-title">
          <h2>Professional</h2>
          <p>Activities</p>
          <h6>Tutorial Organizer: </h6>
          <ul>
            <li>KDD 2022</li>
            <li>KDD 2024</li>
          </ul>

          <h6>Competition Organizer: data leaderboards for (multimodal) LLMs </h6>
          <ul>
            <li><a href="https://tianchi.aliyun.com/competition/entrance/532157">FT-Data Ranker</a></li>
            <li> <a href="https://tianchi.aliyun.com/competition/entrance/532174">BetterMixture</a></li>
            <li><a href="https://tianchi.aliyun.com/competition/entrance/532219">ModelScope-Sora</a></li>
            <li><a href="https://tianchi.aliyun.com/competition/entrance/532251">Better Synth</a></li>
          </ul>

          <h6>Competition Participant:</h6>
          <ul>
            <li>KDD Cup, AutoML-Graph Track, 4/149, 2020 (<a href="https://github.com/joneswong/AutoGraph">our
                solution</a>)</li>
          </ul>

          <h6>Conference Area Chair:</h6>
          <ul>
            <li>NeurIPS (2025)</li>
          </ul>

          <h6>Conference Reviewer: </h6>
          <ul>
            <li>NeurIPS, ICML, ICLR (2022-2025)</li>
            <li>CVPR, ICCV, ECCV (2023-2025)</li>
            <li>COLM (2024-2025)</li>
            <li>KDD (2021-2024)</li>
            <li>ACL, EMNLP, NAACL (2021-2024)</li>
            <li>IJCAI, CIKM (2021-2022)</li>
            </li>
          </ul>
          <h6>Journal Reviewer: </h6>
          <ul>
            <li>Artificial Intelligence (AIJ) </li>
            <li>Journal of Machine Learning Research (JMLR) </li>
            <li>IEEE Transactions on Knowledge and Data Engineering (TKDE) </li>
            <li>IEEE Transactions on Computers (TC) </li>
            <li>Expert Systems with Applications (ESWA) </li>
            <li>IEEE Transactions on Big Data (TBD) </li>
            <li>Knowledge-Based Systems (KBS) </li>
            <li>Artificial Intelligence In Medicine</li>
            <li>Neural Networks</li>
            <li>Neurocomputing</li>
            <li>Patterns</li>
          </ul>

        </div>
      </div>
    </section><!-- End activities Section -->


    <!-- ======= misc Section ======= -->
    <section id="misc" class="faq">
      <div class="container">
        <div class="section-title">
          <h2></h2>
          <p>Misc.</p>

        </div>

        <blockquote>
          <h6>Creativity is intelligence having fun.</h6>
        </blockquote>

        <p>I enjoy learning new things, playing basketball, guitar, and music (especially R&amp;B and hip-hop).</p>

      </div>
    </section><!-- End misc Section -->

    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact">
      <div class="container">

        <div class="section-title">
          <h2></h2>
          <p>Contact</p>
        </div>
        <p>
          Collaborations are welcome; we're currently hiring full-time researchers/developers and self-motivated
          interns!
        </p>
        <p>
          Feel free to reach out if you’re interested: <a
            href="mailto:daoyuanchen.cdy@alibaba-inc.com">daoyuanchen.cdy@alibaba-inc.com</a>, <a
            href="mailto:chendaoyuan@pku.edu.cn">chendaoyuan@pku.edu.cn</a>
        </p>
        <a rel="me" href="https://github.com/yxdyc" target="_blank" title="github">
          <svg>
            <image href="assets/github-icon.png" width="50" height="50" />
          </svg>
        </a>
        <a rel="me" href="https://scholar.google.com/citations?hl=en&user=1GdfinUAAAAJ" target="_blank"
          title="google_scholar">
          <svg>
            <image href="assets/scholar-icon.png" width="50" height="50" />
          </svg>
      </div>

    </section><!-- End Contact Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="credits">
      </div>
    </div>
  </footer>

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>